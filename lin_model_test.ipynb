{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Multilevel Modeling](https://en.wikipedia.org/wiki/Multilevel_model)\n",
    "Idea: Decompose the sum of random numbers into its contributions\n",
    "\n",
    "for a given set of $x_i$ and $a_{ik}$ with\n",
    "$$x_i = \\Sigma_{j=0}^n \\Sigma_{k=1}^{m_j} a_{ik}y_{jk}$$ \n",
    "with\n",
    "* $m_j$ being the number of contributers of layer j\n",
    "* $y_{jk}$ being the k-th contribution of layer j\n",
    "* $m_0 = 1$ by default\n",
    "* $a_{ik} \\in \\{0, 1\\}$\n",
    "* $\\Sigma a_{ik} = 1$ only one contributer per layer\n",
    "\n",
    "calculate the Distributions $y_{k} \\sim N(μ_{k}, σ_{k})$\n",
    "Boundary conditions:\n",
    "* $\\Sigma _{k} μ_k = 0$ via $μ_{m_j} = -\\Sigma_{k = 1}^{m_j-1} μ_{k}$\n",
    "\n",
    "\n",
    "Goal of this workbook to explore [Linear Models](https://scikit-learn.org/stable/modules/linear_model.html) see also [(Examples)](https://scikit-learn.org/stable/auto_examples/linear_model/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_coefficients(coefs):\n",
    "    \"\"\"ensure that the average over all contributers per level is 0\"\"\"\n",
    "    mem = []\n",
    "    for lcoefs in coefs:\n",
    "        avg = np.mean([m for (m, s) in lcoefs.values()])\n",
    "        mem.append({k: (m-avg, s) for k, (m,s) in lcoefs.items()})\n",
    "    \n",
    "    return mem\n",
    "\n",
    "def gen_contributer_coefficients(n_contributers, contributer_avg, contributer_sig):\n",
    "    \"\"\"creates for n_contributers the average and standard deviation\n",
    "    Input:\n",
    "    * n_contributers: list with number of influencers for each level\n",
    "    * contributer_avg: average for the average of the influencers\n",
    "    * contributer_sig: average for the stdev for the influencers (lognormal distribution) \n",
    "     \n",
    "    Output:\n",
    "    Dictionary {level : { influencer: (mu, sigma)}} with the coefficients for every\n",
    "    influencer in each level\n",
    "    \"\"\"\n",
    "    assert n_contributers[0] == 1, \"First level is allowed to have one contributer\"\n",
    "    return [{i : (np.random.normal(contributer_avg), np.random.lognormal(contributer_sig)) for i in range(num)} for lvl, num in enumerate(n_contributers)]\n",
    "    return align_coefficients([{i : (np.random.normal(contributer_avg), np.random.lognormal(contributer_sig)) for i in range(num)} for lvl, num in enumerate(n_contributers)])\n",
    "\n",
    "def print_coefficients(contributer_coefficients):\n",
    "    #contributer_coefficients = align_coefficients(contributer_coefficients)\n",
    "    for m, stage in enumerate(contributer_coefficients):\n",
    "        for i, (mu, sig) in stage.items():\n",
    "            print(f\"Stage {m}: Contributer {i} mu={mu:.2f}, sig={sig:.2f}\")\n",
    "\n",
    "def gen_data(contributer_coefficients, n_samples):\n",
    "    \"\"\"generate random data.\n",
    "    The first level defines the baseline that holds for all random numbers\n",
    "    \n",
    "    Inputs:\n",
    "    * n_samples: number of samples\n",
    "    * contributer_coefficients: dictionary with (mean,sig) per level per contributer\n",
    "    \n",
    "    Outputs:\n",
    "    * data: array with the final number\n",
    "    * contributers: matrix defining the contributers, first column is for the first level\"\"\"\n",
    "    \n",
    "    assert len(contributer_coefficients[0]) == 1, \"Level 0 defines the baseline. It should have exactly one contributer\"\n",
    "    #data = np.random.normal(gen_avg, gen_sig, n_samples)\n",
    "    data = np.zeros((n_samples, ))\n",
    "    contributers = np.zeros((n_samples, len(contributer_coefficients)))\n",
    "    for lvl, cdict in enumerate(contributer_coefficients):\n",
    "        #print(f\"creating level {lvl}\")\n",
    "        lvl_influencers = len(cdict) #number of influencers in this level\n",
    "        lvldata = np.zeros((n_samples, lvl_influencers))\n",
    "\n",
    "        for i, (mu,sig) in cdict.items():\n",
    "            lvldata[:,i] = np.random.normal(mu, sig, n_samples)\n",
    "\n",
    "        selection = np.random.randint(low=0,\n",
    "                                    high=lvl_influencers,\n",
    "                                    size=(n_samples))\n",
    "        contributers[:, lvl] = selection\n",
    "        \n",
    "        data += np.array([lvldata[row, col] for row, col in enumerate(selection)])\n",
    "        # Note: The first level \n",
    "    return data, contributers[:, 1:].astype(int)\n",
    "\n",
    "def generate(n_samples, contributer_coefficients):\n",
    "    if contributer_coefficients == 2:\n",
    "        n_contributers= len(contributer_coefficients[1])\n",
    "        print(\"ATTENTION: just for one level\")\n",
    "    else:\n",
    "        n_contributers = [len(level) for level in contributer_coefficients][1:]\n",
    "\n",
    "    print(\"generating Data for\")\n",
    "    print_coefficients(contributer_coefficients)\n",
    "\n",
    "    data, contributers = gen_data(n_samples=n_samples,\n",
    "                              contributer_coefficients=contributer_coefficients)\n",
    "\n",
    "    #contributers = contributers.squeeze(-1) # kill the first level - not needed\n",
    "    return data, contributers, n_contributers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating Data for\n",
      "Stage 0: Contributer 0 mu=2.00, sig=5.00\n",
      "Stage 1: Contributer 0 mu=2.00, sig=1.00\n",
      "Stage 1: Contributer 1 mu=1.00, sig=1.00\n",
      "Stage 1: Contributer 2 mu=-1.00, sig=1.00\n",
      "Stage 1: Contributer 3 mu=0.00, sig=5.00\n",
      "Stage 1: Contributer 4 mu=-2.00, sig=2.00\n",
      "Stage 2: Contributer 0 mu=3.00, sig=1.00\n",
      "Stage 2: Contributer 1 mu=-1.00, sig=1.00\n",
      "Stage 2: Contributer 2 mu=1.00, sig=1.00\n",
      "Stage 2: Contributer 3 mu=0.00, sig=1.00\n",
      "Stage 2: Contributer 4 mu=-3.00, sig=3.00\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100000\n",
    "\n",
    "cc_27 = [\n",
    "    {    0 : (2, 5)},\n",
    "    {    0: (2, 1),\n",
    "        1: (-1, 1),\n",
    "        2: (-1, 1)},\n",
    "    {    0: (3, 1),\n",
    "        1: (-1, 1),\n",
    "        2: (-1, 1),\n",
    "        3: (-1, 1)}\n",
    "        ]\n",
    "\n",
    "cc_27b = [\n",
    "    {    0 : (2, 5)},\n",
    "    {    0: (2, 1),\n",
    "        1: (-1, 1),\n",
    "        2: (-1, 3)},\n",
    "    {    0: (2, 1),\n",
    "        1: (-1, 1),\n",
    "        2: (-1, 1),\n",
    "        3: (0, 1)}\n",
    "        ]\n",
    "\n",
    "cc_2x = [\n",
    "    {    0 : (2, 5)},\n",
    "    {    0: (2, 1),\n",
    "        1: (1, 1),\n",
    "        2: (-1, 1),\n",
    "        3: (0, 5),\n",
    "        4: (-2, 2),\n",
    "        },\n",
    "    {    0: (3, 1),\n",
    "        1: (-1, 1),\n",
    "        2: (1, 1),\n",
    "        3: (0, 1),\n",
    "        4: (-3, 3)}\n",
    "        ]\n",
    "\n",
    "\n",
    "data, contributers, n_contributers = generate(n_samples, contributer_coefficients=cc_2x)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(contributers)\n",
    "contributers_ohe = enc.transform(contributers).toarray()\n",
    "n_features = np.sum(n_contributers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(coefficients, text = \"\"):\n",
    "\n",
    "    l = list(coefficients)\n",
    "    p = lambda i: [f\"{l.pop(0):.3f}\" for _ in range(i)]\n",
    "    print(text) if text != \"\" else \"\"\n",
    "    \n",
    "    for i,n in enumerate(n_contributers):\n",
    "        print(f\"{i+1}: {p(n)}\")\n",
    "        \n",
    "def eval(regressor, n_contributers=n_contributers):\n",
    "    \"\"\"print intercept and coefficients\"\"\"\n",
    "    print(f\"Intercept: {regressor.intercept_}\")\n",
    "    show(regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ridge Regression](https://scikit-learn.org/stable/modules/linear_model.html#regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 2.016177169262657\n",
      "1: ['1.987', '0.996', '-1.036', '0.029', '-1.976']\n",
      "2: ['2.993', '-1.017', '0.992', '0.005', '-2.974']\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=1, fit_intercept=True)\n",
    "reg.fit(contributers_ohe, data)\n",
    "eval(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Lasso](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "--> [LassoLarsCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html#sklearn.linear_model.LassoLarsCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.886657158656976\n",
      "0: ['2.821', '0.000', '-0.023']\n",
      "1: ['2.301', '-0.866', '-0.838', '0.000']\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LassoLarsCV(cv=5).fit(contributers_ohe, data)\n",
    "reg.score(contributers_ohe, data)\n",
    "eval(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Elastic-Net](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.4958976611423003\n",
      "0: ['2.250', '-0.581', '-0.625']\n",
      "1: ['2.294', '-0.913', '-0.884', '0.000']\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.ElasticNetCV(cv=5).fit(contributers_ohe, data)\n",
    "eval(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LARS-Lasso](https://scikit-learn.org/stable/modules/linear_model.html#lars-lasso)\n",
    "--> finds biggest contributers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.6458608897258027\n",
      "0: ['2.419', '0.000', '0.000']\n",
      "1: ['2.295', '-0.108', '-0.117', '0.000']\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.LassoLars(alpha=0.1).fit(contributers_ohe, data)\n",
    "eval(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Orthogonal Matching Pursuit](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.2624933616660958\n",
      "1: ['0.000', '0.000', '0.000', '0.000', '0.000']\n",
      "2: ['3.732', '0.000', '0.000', '0.000', '0.000']\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.OrthogonalMatchingPursuit().fit(contributers_ohe, data)\n",
    "eval(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Bayesian Ridge Regression](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-ridge-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence after  2  iterations\n",
      "Intercept: 2.0161731641321925\n",
      "1: ['1.986', '0.996', '-1.035', '0.029', '-1.976']\n",
      "2: ['2.992', '-1.016', '0.992', '0.005', '-2.973']\n",
      "precision of the noise: 0.029124163203749852\n",
      "Estimated precision of the weights: 0.26908443276033245\n",
      "Estimated variance-covariance matrix of the weights: [[ 7.44645911e-01  7.42914013e-01  7.42914033e-01  7.42918419e-01\n",
      "   7.42913283e-01  7.71031155e-06  3.73987373e-06  1.60548706e-06\n",
      "  -7.67053611e-06 -5.38513623e-06]\n",
      " [ 7.42914013e-01  7.44634015e-01  7.42918014e-01  7.42922360e-01\n",
      "   7.42917257e-01 -1.02929811e-06 -3.68008544e-06  9.36535070e-06\n",
      "  -3.07135053e-06 -1.58461663e-06]\n",
      " [ 7.42914033e-01  7.42918014e-01  7.44633765e-01  7.42922494e-01\n",
      "   7.42917353e-01 -2.59102211e-06 -8.70947029e-07 -6.21151916e-07\n",
      "   5.33840871e-06 -1.25528766e-06]\n",
      " [ 7.42918419e-01  7.42922360e-01  7.42922494e-01  7.44620637e-01\n",
      "   7.42921748e-01 -3.00640547e-06  3.95433712e-06 -5.72555274e-06\n",
      "   4.65725645e-06  1.20364648e-07]\n",
      " [ 7.42913283e-01  7.42917257e-01  7.42917353e-01  7.42921748e-01\n",
      "   7.44636017e-01 -1.08358585e-06 -3.14317838e-06 -4.62413311e-06\n",
      "   7.46221473e-07  8.10467586e-06]\n",
      " [ 7.71031155e-06 -1.02929811e-06 -2.59102211e-06 -3.00640547e-06\n",
      "  -1.08358585e-06  7.44637228e-01  7.42917893e-01  7.42916025e-01\n",
      "   7.42916030e-01  7.42918482e-01]\n",
      " [ 3.73987373e-06 -3.68008544e-06 -8.70947029e-07  3.95433712e-06\n",
      "  -3.14317838e-06  7.42917893e-01  7.44630884e-01  7.42918099e-01\n",
      "   7.42918186e-01  7.42920596e-01]\n",
      " [ 1.60548706e-06  9.36535070e-06 -6.21151916e-07 -5.72555274e-06\n",
      "  -4.62413311e-06  7.42916025e-01  7.42918099e-01  7.44636543e-01\n",
      "   7.42916277e-01  7.42918714e-01]\n",
      " [-7.67053611e-06 -3.07135053e-06  5.33840871e-06  4.65725645e-06\n",
      "   7.46221473e-07  7.42916030e-01  7.42918186e-01  7.42916277e-01\n",
      "   7.44636321e-01  7.42918845e-01]\n",
      " [-5.38513623e-06 -1.58461663e-06 -1.25528766e-06  1.20364648e-07\n",
      "   8.10467586e-06  7.42918482e-01  7.42920596e-01  7.42918714e-01\n",
      "   7.42918845e-01  7.44629021e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.BayesianRidge(verbose=True).fit(contributers_ohe, data)\n",
    "eval(reg)\n",
    "\n",
    "print(f\"precision of the noise: {reg.alpha_}\")\n",
    "print(f\"Estimated precision of the weights: {reg.lambda_}\")\n",
    "print(f\"Estimated variance-covariance matrix of the weights: {reg.sigma_}\")\n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Auomatic Relevance Determination](https://scikit-learn.org/stable/modules/linear_model.html#automatic-relevance-determination-ard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 3 iterations\n",
      "Intercept: 2.050897629052473\n",
      "1: ['1.957', '0.965', '-1.064', '-0.000', '-2.005']\n",
      "2: ['2.987', '-1.020', '0.985', '0.000', '-2.979']\n",
      "precision of the noise: 0.029124165499521043\n",
      "Estimated precision of the weights:\n",
      "1: ['0.261', '1.069', '0.881', '1035.977', '0.248']\n",
      "2: ['0.112', '0.957', '1.027', '26583.736', '0.113']\n",
      "Estimated variance-covariance matrix of the weights:\n",
      " [[ 4.37612738e-03  2.63662054e-03  2.63736014e-03  9.58574720e-04\n",
      "   2.64021314e-03  2.29445227e-05  1.20456452e-05  1.94821707e-05\n",
      "   6.73949701e-06]\n",
      " [ 2.63662054e-03  4.34669136e-03  2.63376147e-03  9.57252047e-04\n",
      "   2.63660331e-03  9.60550841e-06  5.72142817e-08  2.26242858e-05\n",
      "   5.94922187e-06]\n",
      " [ 2.63736014e-03  2.63376147e-03  4.34843253e-03  9.57561749e-04\n",
      "   2.63741908e-03 -3.10165997e-07 -5.48608312e-06  4.32341613e-06\n",
      "  -2.07806092e-06]\n",
      " [ 9.58574720e-04  9.57252047e-04  9.57561749e-04  9.62999254e-04\n",
      "   9.58599317e-04 -1.68120308e-08  2.33471621e-09 -3.31027367e-08\n",
      "  -8.91592115e-09]\n",
      " [ 2.64021314e-03  2.63660331e-03  2.63741908e-03  9.58599317e-04\n",
      "   4.35974635e-03  5.78227270e-06 -3.18340500e-06  4.90786385e-06\n",
      "   1.18673642e-05]\n",
      " [ 2.29445227e-05  9.60550841e-06 -3.10165997e-07 -1.68120308e-08\n",
      "   5.78227270e-06  3.43558383e-03  1.71115491e-03  1.71097574e-03\n",
      "   1.71355802e-03]\n",
      " [ 1.20456452e-05  5.72142817e-08 -5.48608312e-06  2.33471621e-09\n",
      "  -3.18340500e-06  1.71115491e-03  3.41750786e-03  1.70842418e-03\n",
      "   1.71104340e-03]\n",
      " [ 1.94821707e-05  2.26242858e-05  4.32341613e-06 -3.31027367e-08\n",
      "   4.90786385e-06  1.71097574e-03  1.70842418e-03  3.42631807e-03\n",
      "   1.71085067e-03]\n",
      " [ 6.73949701e-06  5.94922187e-06 -2.07806092e-06 -8.91592115e-09\n",
      "   1.18673642e-05  1.71355802e-03  1.71104340e-03  1.71085067e-03\n",
      "   3.42173971e-03]]\n",
      "Scores\n",
      "[-226829.81210764046, -226831.6976709101, -226831.6824497583, -226831.8077994442]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.ARDRegression(verbose=True,\n",
    "                                 compute_score=True,\n",
    "                                 ).fit(contributers_ohe, data)\n",
    "eval(reg)\n",
    "\n",
    "print(f\"precision of the noise: {reg.alpha_}\")\n",
    "\n",
    "show(reg.lambda_, \"Estimated precision of the weights:\")\n",
    "\n",
    "print(f\"Estimated variance-covariance matrix of the weights:\\n {reg.sigma_}\")\n",
    "\n",
    "print(\"Scores\")\n",
    "print(reg.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 0: Contributer 0 mu=2.00, sig=5.00\n",
      "Stage 1: Contributer 0 mu=2.00, sig=1.00\n",
      "Stage 1: Contributer 1 mu=1.00, sig=1.00\n",
      "Stage 1: Contributer 2 mu=-1.00, sig=1.00\n",
      "Stage 1: Contributer 3 mu=0.00, sig=5.00\n",
      "Stage 1: Contributer 4 mu=-2.00, sig=2.00\n",
      "Stage 2: Contributer 0 mu=3.00, sig=1.00\n",
      "Stage 2: Contributer 1 mu=-1.00, sig=1.00\n",
      "Stage 2: Contributer 2 mu=1.00, sig=1.00\n",
      "Stage 2: Contributer 3 mu=0.00, sig=1.00\n",
      "Stage 2: Contributer 4 mu=-3.00, sig=3.00\n"
     ]
    }
   ],
   "source": [
    "print_coefficients(cc_2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 2.0509461800468456\n",
      "1: ['1.957', '0.965', '-1.064', '-0.000', '-2.005']\n",
      "2: ['2.987', '-1.020', '0.985', '-0.000', '-2.979']\n",
      "precision of the noise: 0.02912416549742874\n",
      "Estimated precision of the weights:\n",
      "1: ['0.261', '1.069', '0.881', '1100.300', '0.248']\n",
      "2: ['0.112', '0.957', '1.026', '614.879', '0.113']\n",
      "Estimated variance-covariance matrix of the weights:\n",
      " [[ 4.32047135e-03  2.58104150e-03  2.58176312e-03  9.02661311e-04\n",
      "   2.58455568e-03  2.28892773e-05  1.19893781e-05  1.94279797e-05\n",
      "  -5.64771312e-08  6.68379226e-06]\n",
      " [ 2.58104150e-03  4.29118935e-03  2.57824137e-03  9.01415861e-04\n",
      "   2.58102286e-03  9.56611947e-06  1.67705507e-08  2.25859292e-05\n",
      "  -4.05483189e-08  5.90937406e-06]\n",
      " [ 2.58176312e-03  2.57824137e-03  4.29289463e-03  9.01707501e-04\n",
      "   2.58182065e-03 -3.07448950e-07 -5.48448668e-06  4.32708195e-06\n",
      "   1.74867525e-09 -2.07580273e-06]\n",
      " [ 9.02661311e-04  9.01415861e-04  9.01707501e-04  9.06827600e-04\n",
      "   9.02684477e-04 -1.57783163e-08  2.25082112e-09 -3.11181339e-08\n",
      "   5.28378244e-11 -8.34302967e-09]\n",
      " [ 2.58455568e-03  2.58102286e-03  2.58182065e-03  9.02684477e-04\n",
      "   4.30408749e-03  5.77683234e-06 -3.18995294e-06  4.90338619e-06\n",
      "  -6.44540333e-09  1.18614649e-05]\n",
      " [ 2.28892773e-05  9.56611947e-06 -3.07448950e-07 -1.57783163e-08\n",
      "   5.77683234e-06  5.04329118e-03  3.31654493e-03  3.31615968e-03\n",
      "   1.61411377e-03  3.32126648e-03]\n",
      " [ 1.19893781e-05  1.67705507e-08 -5.48448668e-06  2.25082112e-09\n",
      "  -3.18995294e-06  3.31654493e-03  5.02058548e-03  3.31129443e-03\n",
      "   1.61178563e-03  3.31643454e-03]\n",
      " [ 1.94279797e-05  2.25859292e-05  4.32708195e-06 -3.11181339e-08\n",
      "   4.90338619e-06  3.31615968e-03  3.31129443e-03  5.02898418e-03\n",
      "   1.61157872e-03  3.31603573e-03]\n",
      " [-5.64771312e-08 -4.05483189e-08  1.74867525e-09  5.28378244e-11\n",
      "  -6.44540333e-09  1.61411377e-03  1.61178563e-03  1.61157872e-03\n",
      "   1.62054901e-03  1.61411489e-03]\n",
      " [ 6.68379226e-06  5.90937406e-06 -2.07580273e-06 -8.34302967e-09\n",
      "   1.18614649e-05  3.32126648e-03  3.31643454e-03  3.31603573e-03\n",
      "   1.61411489e-03  5.02944933e-03]]\n",
      "Scores\n",
      "[-226830.18472031958, -226840.09391626646, -226833.5958880388, -226839.88545092265, -226833.89092779742, -226839.66549486984, -226834.07150979608, -226839.50988238465, -226834.20549756353, -226839.38845089142, -226834.313104062, -226839.28839924638, -226834.4034701761, -226839.20306061162, -226834.48160194702, -226839.128501619, -226834.55056451383, -226839.06219917937, -226834.61238033875, -226839.0024344034, -226834.6684581748, -226838.94798170472, -226834.71982063906, -226838.89793543893, -226834.7672346476, -226838.85160681626, -226834.81129076856, -226838.80845938163, -226834.8524538536, -226838.76806691117, -226834.89109662737, -226838.73008496387, -226834.92752270872, -226838.69423108528, -226834.96198283156, -226838.66027067954, -226834.99468654883, -226838.62800670255, -226835.02581085486, -226838.59727199798, -226835.05550665307, -226838.5679234998, -226835.0839036874, -226838.53983778052, -226835.11111435533, -226838.51290758612, -226835.13723669623, -226838.4870391053, -226835.1623567595, -226838.4621497942, -226835.1865505016, -226838.4381666254, -226835.2098853209, -226838.41502466588, -226835.2324213096, -226838.39266591182, -226835.2542122845, -226838.37103832673, -226835.2753066403, -226838.35009504194, -226835.29574806298, -226838.32979368657, -226835.31557612834, -226838.31009582366, -226835.33482680816, -226838.29096647227, -226835.35353290028, -226838.27237370028, -226835.37172439587, -226838.25428827576, -226835.38942879543, -226838.23668336717, -226835.40667138013, -226838.21953428414, -226835.4234754484, -226838.20281825238, -226835.4398625202, -226838.18651421808, -226835.45585251675, -226838.1706026762, -226835.47146391677, -226838.15506552003, -226835.486713895, -226838.13988590887, -226835.50161844387, -226838.12504815083, -226835.5161924811, -226838.11053759922, -226835.53044994554, -226838.09634056012, -226835.5444038822, -226838.08244421036, -226835.55806651866, -226838.06883652412, -226835.57144933275, -226838.05550620687, -226835.5845631136, -226838.04244263645, -226835.59741801713, -226838.02963580994, -226835.61002361483, -226838.01707629542, -226835.62238893882, -226838.0047551888, -226835.6345225225, -226837.9926640744, -226835.64643243694, -226837.9807949892, -226835.65812632453, -226837.96914039052, -226835.6696114292, -226837.95769312634, -226835.68089462406, -226837.9464464083, -226835.69198243707, -226837.9353937872, -226835.70288107384, -226837.92452913008, -226835.71359643905, -226837.9138465998, -226835.72413415604, -226837.90334063573, -226835.73449958436, -226837.89300593658, -226835.74469783684, -226837.8828374438, -226835.7547337944, -226837.87283032708, -226835.7646121201, -226837.86297997035, -226835.7743372726, -226837.85328195937, -226835.78391351755, -226837.84373206954, -226835.7933449391, -226837.8343262553, -226835.80263545032, -226837.82506063976, -226835.8117888026, -226837.8159315054, -226835.8208085945, -226837.8069352855, -226835.82969828052, -226837.79806855545, -226835.83846117838, -226837.78932802565, -226835.84710047647, -226837.7807105341, -226835.8556192406, -226837.77221303998, -226835.86402042035, -226837.76383261723, -226835.87230685478, -226837.75556644885, -226835.88048127823, -226837.7474118215, -226835.88854632532, -226837.73936612028, -226835.8965045359, -226837.73142682406, -226835.90435835958, -226837.72359150078, -226835.91211016005, -226837.7158578036, -226835.91976221922, -226837.70822346653, -226835.9273167407, -226837.70068630078, -226835.93477585382, -226837.69324419159, -226835.94214161663, -226837.68589509412, -226835.94941601926, -226837.6786370313, -226835.956600987, -226837.67146808997, -226835.96369838284, -226837.6643864187, -226835.97071001062, -226837.65739022472, -226835.97763761712, -226837.6504777719, -226835.98448289465, -226837.6436473778, -226835.99124748347, -226837.63689741178, -226835.99793297367, -226837.63022629282, -226836.00454090734, -226837.62363248746, -226836.0110727806, -226837.61711450794, -226836.01753004536, -226837.61067091016, -226836.02391411105, -226837.6043002923, -226836.03022634634, -226837.5980012929, -226836.0364680806, -226837.59177258945, -226836.0426406056, -226837.5856128968, -226836.0487451768, -226837.5795209658, -226836.05478301478, -226837.57349558195, -226836.06075530656, -226837.56753556398, -226836.06666320676, -226837.56163976283, -226836.07250783866, -226837.55580706042, -226836.0782902958, -226837.55003636837, -226836.08401164255, -226837.54432662722, -226836.08967291546, -226837.53867680507, -226836.095275124, -226837.53308589698, -226836.10081925182, -226837.52755292365, -226836.10630625716, -226837.52207693082, -226836.11173707427, -226837.5166569883, -226836.117112614, -226837.51129218916, -226836.12243376428, -226837.50598164904, -226836.12770139147, -226837.50072450514, -226836.1329163406, -226837.49551991583, -226836.13807943632, -226837.4903670598, -226836.14319148342, -226837.48526513524, -226836.14825326754, -226837.48021335958, -226836.15326555597, -226837.4752109687, -226836.15822909767, -226837.47025721607, -226836.16314462462, -226837.46535137278, -226836.16801285167, -226837.4604927263, -226836.17283447753, -226837.45568058052, -226836.17761018503, -226837.45091425517, -226836.1823406416, -226837.446193085, -226836.18702649983, -226837.44151641964, -226836.1916683981, -226837.43688362313, -226836.1962669605, -226837.43229407325, -226836.20082279766, -226837.42774716148, -226836.2053365071, -226837.42324229237, -226836.20980867345, -226837.41877888294, -226836.21423986895, -226837.41435636286, -226836.21863065386, -226837.40997417382, -226836.22298157663, -226837.40563176892, -226836.22729317428, -226837.40132861276, -226836.2315659729, -226837.397064181, -226836.2358004877, -226837.39283795975, -226836.23999722354, -226837.3886494458, -226836.24415667495, -226837.38449814596, -226836.24827932683, -226837.38038357696, -226836.25236565416, -226837.3763052649, -226836.25641612278, -226837.37226274534, -226836.26043118935, -226837.36825556296, -226836.26441130158, -226837.36428327113, -226836.26835689868, -226837.36034543204, -226836.27226841144, -226837.3564416159, -226836.2761462623, -226837.35257140134, -226836.27999086594, -226837.34873437491]\n"
     ]
    }
   ],
   "source": [
    "reg = linear_model.ARDRegression(verbose=True,\n",
    "                                 compute_score=True,\n",
    "                                 threshold_lambda=1e6, \n",
    "                                 tol=1e-8\n",
    "                                 ).fit(contributers_ohe, data)\n",
    "eval(reg)\n",
    "\n",
    "print(f\"precision of the noise: {reg.alpha_}\")\n",
    "\n",
    "show(reg.lambda_, \"Estimated precision of the weights:\")\n",
    "\n",
    "print(f\"Estimated variance-covariance matrix of the weights:\\n {reg.sigma_}\")\n",
    "\n",
    "print(\"Scores\")\n",
    "print(reg.scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.zeros((n_features, n_features))\n",
    "np.fill_diagonal(D, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.98313636, -0.02101741, -0.02347963,  3.02407272, -0.02733895,\n",
       "        -0.01941585,  0.97741452]),\n",
       " array([5.45582635, 5.45570748, 5.45570759, 5.45587363, 5.45572356,\n",
       "        5.45571468, 5.45587398]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.predict(D, return_std=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Quantile Regressor](https://scikit-learn.org/stable/auto_examples/linear_model/plot_quantile_regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    if quantile == min(quantiles):\\n        out_bounds_predictions = np.logical_or(\\n            out_bounds_predictions, y_pred >= y_normal\\n        )\\n    elif quantile == max(quantiles):\\n        out_bounds_predictions = np.logical_or(\\n            out_bounds_predictions, y_pred <= y_normal\\n        )\\n        '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = contributers_ohe\n",
    "y = data\n",
    "from sklearn.utils.fixes import parse_version, sp_version\n",
    "\n",
    "# This is line is to avoid incompatibility if older SciPy version.\n",
    "# You should use `solver=\"highs\"` with recent version of SciPy.\n",
    "solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n",
    "\n",
    "from sklearn.linear_model import QuantileRegressor\n",
    "\n",
    "quantiles = [0.05, 0.5, 0.95]\n",
    "predictions = {}\n",
    "#out_bounds_predictions = np.zeros_like(y_true_mean, dtype=np.bool_)\n",
    "for quantile in quantiles:\n",
    "    qr = QuantileRegressor(quantile=quantile, alpha=0, solver=solver)\n",
    "    y_pred = qr.fit(X, y).predict(X)\n",
    "    predictions[quantile] = y_pred\n",
    "\"\"\"\n",
    "    if quantile == min(quantiles):\n",
    "        out_bounds_predictions = np.logical_or(\n",
    "            out_bounds_predictions, y_pred >= y_normal\n",
    "        )\n",
    "    elif quantile == max(quantiles):\n",
    "        out_bounds_predictions = np.logical_or(\n",
    "            out_bounds_predictions, y_pred <= y_normal\n",
    "        )\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> dont use the quantile regressor for this purpose\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "If all standard deviations are the same within one level best Results are obtained by the Ridge Regression and the Bayesian Ridge Regression . If the standard deviation changes, the estimation gets artefacts.\n",
    "\n",
    "If the means and the standard deviations are more distributed, then the ARDRegressor delivers the best estimates. The Estimated Precision of the weights correlates (a bit) to the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
